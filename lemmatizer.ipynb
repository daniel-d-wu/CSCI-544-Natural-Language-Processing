{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wordform types': 16879, 'Wordform tokens': 281057, 'Unambiguous types': 16465, 'Unambiguous tokens': 196204, 'Ambiguous types': 414, 'Ambiguous tokens': 84853, 'Ambiguous most common tokens': 75667, 'Identity tokens': 201485}\n",
      "0.7168830521922599\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Daniel Wu\n",
    "CSCI 544\n",
    "Programming Assignment 2\n",
    "\"\"\"\n",
    "\n",
    "def pause():\n",
    "    programPause = input(\"Press the <ENTER> key to continue...\")\n",
    "    print(\"Paused Program\")\n",
    "\n",
    "# Ron Artstein Notes below:\n",
    "\n",
    "### This program is a very simple lemmatizer, which learns a\n",
    "### lemmatization function from an annotated corpus. The function is\n",
    "### so basic I wouldn't even consider it machine learning: it's\n",
    "### basically just a big lookup table, which maps every word form\n",
    "### attested in the training data to the most common lemma associated\n",
    "### with that form. At test time, the program checks if a form is in\n",
    "### the lookup table, and if so, it gives the associated lemma; if the\n",
    "### form is not in the lookup table, it gives the form itself as the\n",
    "### lemma (identity mapping).\n",
    "\n",
    "### The program performs training and testing in one run: it reads the\n",
    "### training data, learns the lookup table and keeps it in memory,\n",
    "### then reads the test data, runs the testing, and reports the\n",
    "### results.\n",
    "\n",
    "### The program takes two command line arguments, which are the paths\n",
    "### to the training and test files. Both files are assumed to be\n",
    "### already tokenized, in Universal Dependencies format, that is: each\n",
    "### token on a separate line, each line consisting of fields separated\n",
    "### by tab characters, with word form in the second field, and lemma\n",
    "### in the third field. Tab characters are assumed to occur only in\n",
    "### lines corresponding to tokens; other lines are ignored.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import operator\n",
    "\n",
    "### Global variables\n",
    "\n",
    "# Paths for data are read from command line\n",
    "# train_file = sys.argv[1]\n",
    "# test_file = sys.argv[2]\n",
    "\n",
    "train_file = \"/Users/user/Desktop/Fall_2020/CSCI_544/Coding_Assignments/PA2/UD_Hindi-HDTB-master/hi_hdtb-ud-train.conllu\"\n",
    "test_file = \"/Users/user/Desktop/Fall_2020/CSCI_544/Coding_Assignments/PA2/UD_Hindi-HDTB-master/hi_hdtb-ud-test.conllu\"\n",
    "\n",
    "\n",
    "# Counters for lemmas in the training data: word form -> lemma -> count\n",
    "lemma_count = {}\n",
    "\n",
    "# Lookup table learned from the training data: word form -> lemma\n",
    "lemma_max = {}\n",
    "\n",
    "# Variables for reporting results\n",
    "training_stats = ['Wordform types' , 'Wordform tokens' , 'Unambiguous types' , \n",
    "                  'Unambiguous tokens' , 'Ambiguous types' , 'Ambiguous tokens' , \n",
    "                  'Ambiguous most common tokens' , 'Identity tokens']\n",
    "\n",
    "training_counts = dict.fromkeys(training_stats , 0)\n",
    "\n",
    "test_outcomes = ['Total test items' , 'Found in lookup table' , 'Lookup match' , \n",
    "                 'Lookup mismatch' , 'Not found in lookup table' , 'Identity match' , \n",
    "                 'Identity mismatch']\n",
    "\n",
    "test_counts = dict.fromkeys(test_outcomes , 0)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "### Training: read training data and populate lemma counters\n",
    "\n",
    "train_data = open (train_file , 'r')\n",
    "\n",
    "token_count = 0\n",
    "type_count = 0\n",
    "\n",
    "for line in train_data:\n",
    "    \n",
    "    \n",
    "    # Tab character identifies lines containing tokens\n",
    "    if re.search('\\t' , line):\n",
    "        \n",
    "        token_count += 1\n",
    "        \n",
    "        # Tokens represented as tab-separated fields\n",
    "        field = line.strip().split('\\t')\n",
    "\n",
    "        # Word form in second field, lemma in third field\n",
    "        form = field[1]\n",
    "        lemma = field[2]\n",
    "        \n",
    "        ######################################################\n",
    "        ### Insert code for populating the lemma counts    ###\n",
    "        ######################################################                        \n",
    "        \n",
    "        # If new word form\n",
    "        if form not in lemma_count.keys():\n",
    "            lemma_count[form] = {}            \n",
    "            type_count += 1\n",
    "        \n",
    "        #if new lemma\n",
    "        if lemma not in lemma_count[form].keys():            \n",
    "            lemma_count[form][lemma] = 1\n",
    "                \n",
    "        elif lemma in lemma_count[form].keys():            \n",
    "            lemma_count[form][lemma] += 1  \n",
    "\n",
    "            \n",
    "ambig_token_count = 0\n",
    "ambig_type_count = 0\n",
    "unambig_token_count = 0\n",
    "unambig_type_count = 0\n",
    "\n",
    "identity_token_count = 0\n",
    "\n",
    "ambig_most_common_count = 0 \n",
    "\n",
    "\n",
    "### Model building and training statistics\n",
    "\n",
    "for form in lemma_count.keys():\n",
    "\n",
    "    ######################################################\n",
    "    ### Insert code for building the lookup table      ###\n",
    "    ######################################################\n",
    "\n",
    "    lemma_max[form] = max(lemma_count[form].items(), key = operator.itemgetter(1))[0]\n",
    "    \n",
    "    if len(lemma_count[form]) > 1:\n",
    "        ambig_token_count = ambig_token_count + sum(v for v in lemma_count[form].values())\n",
    "        \n",
    "        ambig_type_count += 1\n",
    "            \n",
    "        most_common_count = max(lemma_count[form].items(), key = operator.itemgetter(1))[1]        \n",
    "        \n",
    "        ambig_most_common_count = ambig_most_common_count + most_common_count\n",
    "        \n",
    "    if form in lemma_count[form]:\n",
    "        identity_token_count = identity_token_count + lemma_count[form][form]\n",
    "        \n",
    "unambig_token_count = token_count - ambig_token_count\n",
    "unambig_type_count = type_count - ambig_type_count    \n",
    "\n",
    "######################################################\n",
    "### Insert code for populating the training counts ###\n",
    "######################################################\n",
    "\n",
    "training_counts['Wordform types'] = type_count        \n",
    "training_counts['Wordform tokens'] = token_count\n",
    "training_counts['Unambiguous types'] = unambig_type_count\n",
    "training_counts['Unambiguous tokens'] = unambig_token_count\n",
    "training_counts['Ambiguous types'] = ambig_type_count         \n",
    "training_counts['Ambiguous tokens'] = ambig_token_count\n",
    "training_counts['Ambiguous most common tokens'] = ambig_most_common_count\n",
    "training_counts['Identity tokens'] = identity_token_count\n",
    "        \n",
    "### Calculate expected accuracy if we used lookup on all items ###\n",
    "accuracies['Expected lookup'] = (unambig_token_count + ambig_most_common_count) / token_count\n",
    "\n",
    "### Calculate expected accuracy if we used identity mapping on all items ###\n",
    "accuracies['Expected identity'] = identity_token_count / token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total test items': 35430, 'Found in lookup table': 33849, 'Lookup match': 32628, 'Lookup mismatch': 1221, 'Not found in lookup table': 1581, 'Identity match': 1227, 'Identity mismatch': 354}\n",
      "{'Expected lookup': 0.9673162383431119, 'Expected identity': 0.7168830521922599, 'Lookup': 0.9639280333244704, 'Identity': 0.7760910815939279, 'Overall': 0.9555461473327689}\n"
     ]
    }
   ],
   "source": [
    "### Testing: read test data, and compare lemmatizer output to actual lemma\n",
    "    \n",
    "test_data = open (test_file , 'r')\n",
    "\n",
    "test_item_count = 0\n",
    "found_count = 0\n",
    "lookup_match_count = 0\n",
    "\n",
    "id_match_count = 0\n",
    "id_mismatch_count = 0\n",
    "\n",
    "for line in test_data:\n",
    "\n",
    "    # Tab character identifies lines containing tokens\n",
    "    if re.search ('\\t' , line):\n",
    "\n",
    "        # Tokens represented as tab-separated fields\n",
    "        field = line.strip().split('\\t')\n",
    "\n",
    "        # Word form in second field, lemma in third field\n",
    "        form = field[1]\n",
    "        lemma = field[2]\n",
    "\n",
    "        ######################################################\n",
    "        ### Insert code for populating the test counts     ###\n",
    "        ######################################################\n",
    "        \n",
    "        test_item_count += 1\n",
    "        \n",
    "        if form in lemma_max:            \n",
    "            found_count += 1\n",
    "            \n",
    "            if lemma == lemma_max[form]:                \n",
    "                lookup_match_count += 1\n",
    "            \n",
    "        elif form not in lemma_max:\n",
    "            \n",
    "            if form == lemma:                                \n",
    "                    id_match_count += 1\n",
    "            elif form != lemma:\n",
    "                id_mismatch_count += 1\n",
    "                        \n",
    "not_found_count = test_item_count - found_count\n",
    "\n",
    "lookup_mismatch_count = found_count - lookup_match_count\n",
    "        \n",
    "    \n",
    "test_counts['Total test items'] = test_item_count \n",
    "test_counts['Found in lookup table'] = found_count \n",
    "test_counts['Lookup match'] = lookup_match_count\n",
    "test_counts['Lookup mismatch'] = lookup_mismatch_count\n",
    "test_counts['Not found in lookup table'] = not_found_count\n",
    "test_counts['Identity match'] = id_match_count\n",
    "test_counts['Identity mismatch'] = id_mismatch_count\n",
    "\n",
    "\n",
    "### Calculate accuracy on the items that used the lookup table ###\n",
    "accuracies['Lookup'] = lookup_match_count / found_count\n",
    "\n",
    "### Calculate accuracy on the items that used identity mapping ###\n",
    "accuracies['Identity'] = id_match_count / (id_match_count + id_mismatch_count)\n",
    "\n",
    "### Calculate overall accuracy ###\n",
    "accuracies['Overall'] = (lookup_match_count + id_match_count) / test_item_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Report training statistics and test results\n",
    "                \n",
    "output = open('lookup-output.txt' , 'w')\n",
    "\n",
    "output.write('Training statistics\\n')\n",
    "\n",
    "for stat in training_stats:\n",
    "    output.write(stat + ': ' + str(training_counts[stat]) + '\\n')\n",
    "\n",
    "for model in ['Expected lookup' , 'Expected identity']:\n",
    "    output.write(model + ' accuracy: ' + str(accuracies[model]) + '\\n')\n",
    "\n",
    "output.write ('Test results\\n')\n",
    "\n",
    "for outcome in test_outcomes:\n",
    "    output.write(outcome + ': ' + str(test_counts[outcome]) + '\\n')\n",
    "\n",
    "for model in ['Lookup' , 'Identity' , 'Overall']:\n",
    "    output.write(model + ' accuracy: ' + str(accuracies[model]) + '\\n')\n",
    "\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
