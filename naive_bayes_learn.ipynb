{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Daniel Wu\n",
    "Purpose: PS3 - Train a bernoulli naive bayes model\n",
    "         on hotel reviews\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "\n",
    "def pause():\n",
    "    programPause = input(\"Press the <ENTER> key to continue...\")\n",
    "    print(\"Paused Program\")\n",
    "    \n",
    "# root_dir = sys.argv[1]\n",
    "root_dir = \"/Users/user/Desktop/Fall_2020/CSCI_544/Coding_Assignments/PA3/op_spam_training_data\"\n",
    "\n",
    "# filepath dictionary\n",
    "p = {}\n",
    "\n",
    "p['nd'] = \"/negative_polarity/deceptive_from_MTurk\"\n",
    "p['nt'] = \"/negative_polarity/truthful_from_Web\"\n",
    "p['pd'] = \"/positive_polarity/deceptive_from_MTurk\"\n",
    "p['pt'] = \"/positive_polarity/truthful_from_TripAdvisor\"\n",
    "\n",
    "def store_reviews(sub_dir):\n",
    "    \n",
    "    review_list = []\n",
    "    \n",
    "    for path in list(os.walk(root_dir + sub_dir))[1:]:    \n",
    "        for text in path[2]:        \n",
    "            file_path = path[0] + \"/\" + text\n",
    "        \n",
    "            with open(file_path) as doc:                \n",
    "                review_list.append(''.join(doc.readlines()))\n",
    "        \n",
    "    return review_list\n",
    "\n",
    "# review dictionary \n",
    "reviews = {}\n",
    "\n",
    "for sub_dir in ['nd', 'nt', 'pd', 'pt']:                \n",
    "        reviews[sub_dir] = store_reviews(p[sub_dir])\n",
    "\n",
    "        \n",
    "# set stop words - hotel context\n",
    "#                  first / second person pronouns\n",
    "#                  common filler words\n",
    "\n",
    "# some of these are also retroactively added in\n",
    "# based on joint probabilities being too high\n",
    "\n",
    "stop_words = ['hotel', 'hotels', 'stay', 'stayed',\n",
    "              'book', 'booked', 'reserve', 'reserved',\n",
    "              'room', 'rooms',\n",
    "              'reservation', 'here',\n",
    "              'i', 'me', 'my', 'mine',\n",
    "              'the', 'we', 'our', 'ours',\n",
    "              'it', 'its', 'they', 'them',\n",
    "              'he', 'she', 'him', 'her', 'his',\n",
    "              'they', 'them', 'theirs', 'who', 'what', 'where',\n",
    "              'when', 'am', 'are', 'about',\n",
    "              'to', 'in', 'out', 'up', 'down',\n",
    "              'a', 'an', 'how', 'if', 'as', 'on',\n",
    "              'some', 'can', 'is', 'be', 'any', \n",
    "              'through', 'of', 'off',\n",
    "              'these', 'those', 'that',              \n",
    "              'one', 'ha', 'would', 'from', 'by', 'thing',\n",
    "              'this', 'and', 'for', ' ', 'during', 'before',\n",
    "              'after', 'very'\n",
    "              \"i'll\", \"we'll\", \"it's\",\n",
    "              \"i'm\"\n",
    "             ]\n",
    "\n",
    "\n",
    "puncs1 = string.punctuation.replace(\"'\", '')\n",
    "puncs2 = puncs1.replace(\"-\", '')\n",
    "puncs = list(puncs2)\n",
    "\n",
    "#get list of tokens - seperate by space \" \"\n",
    "#generalize time,\n",
    "#generalize amount,\n",
    "#separate punctuation\n",
    "\n",
    "token_bag = {}\n",
    "clean_reviews = {}\n",
    "\n",
    "for cls in ['nd', 'nt', 'pd', 'pt']:\n",
    "    \n",
    "    word_list = \"\"    \n",
    "    clean_reviews[cls] = []\n",
    "    \n",
    "    for review in reviews[cls]:        \n",
    "                        \n",
    "        review = re.sub(r\"(?:[0-2]?[0-9])(?:(?:am|pm)|(?::[0-5][0-9]?)(?:am|pm)?)\", \"timetok\", review)        \n",
    "        review = re.sub(r\"\\$\\d+(?:\\.\\d?\\d)?\", \"amttok\", review)        \n",
    "        review = review.translate(str.maketrans({punc: \" {0} \".format(punc) for punc in puncs}))\n",
    "                         \n",
    "        word_list = word_list + review.lower()\n",
    "        \n",
    "        clean_reviews[cls].append(review.lower())\n",
    "                    \n",
    "    token_bag[cls] = set(word_list.split(' '))\n",
    "    \n",
    "    # remove stop words and punctuations\n",
    "    token_bag[cls] = [tok for tok in token_bag[cls] if tok not in stop_words]\n",
    "    \n",
    "    #get rid of letters and 2-letter words, but keep a few punctuations \n",
    "    token_bag[cls] = [tok for tok in token_bag[cls] if (len(tok) > 2 or tok in ('?', '!'))]\n",
    "    \n",
    "    \n",
    "# Create total bag of words\n",
    "token_bag['total'] = set(token_bag['nd'] + token_bag['nt'] + token_bag['pd'] + token_bag['pt'])\n",
    "\n",
    "# get counts in the 4-class classifier    \n",
    "token_count = {}\n",
    "\n",
    "for cls in ['nd', 'nt', 'pd', 'pt']:\n",
    "    \n",
    "    token_count[cls] = {}    \n",
    "            \n",
    "    for tok in token_bag['total']:            \n",
    "        tok_count = len([1 for review in clean_reviews[cls] if tok in review])                                        \n",
    "        token_count[cls][tok] = {}\n",
    "        token_count[cls][tok] = tok_count\n",
    "        \n",
    "        \n",
    "# get joint prob\n",
    "joint_prob_pn = {} #for pos/neg\n",
    "joint_prob_td = {} #for true/deceptive\n",
    "\n",
    "for tok in token_bag['total']:\n",
    "        \n",
    "    # first element is joint-prob for positive\n",
    "    # second element is joint-prob for negative\n",
    "    \n",
    "    joint_prob_pn[tok] = [ (token_count['pd'][tok] + token_count['pt'][tok] + 1 ) / \n",
    "                           (len(clean_reviews['pd']) + len(clean_reviews['pt']) + 1),\n",
    "                           (token_count['nd'][tok] + token_count['nt'][tok] + 1 ) / \n",
    "                           (len(clean_reviews['nd']) + len(clean_reviews['nt']) + 1) ]          \n",
    "    \n",
    "    # first element is joint-prob for true\n",
    "    # second element is joint-prob for deceptive\n",
    "    \n",
    "    joint_prob_td[tok] = [ (token_count['pt'][tok] + token_count['nt'][tok] + 1 ) / \n",
    "                           (len(clean_reviews['pt']) + len(clean_reviews['nt']) + 1),\n",
    "                           (token_count['pd'][tok] + token_count['nd'][tok] + 1 ) / \n",
    "                           (len(clean_reviews['pd']) + len(clean_reviews['nd']) + 1) ]\n",
    "\n",
    "    \n",
    "# trim the list to avoid overfit\n",
    "\n",
    "joint_prob_pn = list(joint_prob_pn.items())\n",
    "joint_prob_pn.sort(key= lambda x: x[1][0] + x[1][1], reverse=True)\n",
    "joint_prob_pn = joint_prob_pn[0:1000]\n",
    "\n",
    "# more tokens for td since true reviews may have wider vocab\n",
    "\n",
    "joint_prob_td = list(joint_prob_td.items())\n",
    "joint_prob_td.sort(key= lambda x: x[1][0] + x[1][1], reverse=True)\n",
    "joint_prob_td = joint_prob_td[0:3000]\n",
    "\n",
    "p_prior = (len(clean_reviews['pd']) + len(clean_reviews['pt']))/((len(clean_reviews['pd']) + len(clean_reviews['pt'])) + len(clean_reviews['nd']) + len(clean_reviews['nt']))\n",
    "n_prior = (len(clean_reviews['nd']) + len(clean_reviews['nt']))/((len(clean_reviews['pd']) + len(clean_reviews['pt'])) + len(clean_reviews['nd']) + len(clean_reviews['nt']))\n",
    "t_prior = (len(clean_reviews['pt']) + len(clean_reviews['nt']))/((len(clean_reviews['pt']) + len(clean_reviews['nt'])) + len(clean_reviews['pd']) + len(clean_reviews['nd']))\n",
    "d_prior = (len(clean_reviews['pd']) + len(clean_reviews['nd']))/((len(clean_reviews['pt']) + len(clean_reviews['nt'])) + len(clean_reviews['pd']) + len(clean_reviews['nd']))\n",
    "\n",
    "# {class} {word} {joint_prob_1} {joint_prob_2}\n",
    "\n",
    "outfile = \"\"\n",
    "\n",
    "outfile += f\"pn PRIORS {p_prior} {n_prior} \\n\"\n",
    "\n",
    "for tok in joint_prob_pn:\n",
    "    outfile += f\"pn {tok[0]} {tok[1][0]} {tok[1][1]} \\n\"\n",
    "    \n",
    "outfile += f\"td PRIORS {t_prior} {d_prior} \\n\"\n",
    "\n",
    "for tok in joint_prob_td:\n",
    "    outfile += f\"td {tok[0]} {tok[1][0]} {tok[1][1]} \\n\"\n",
    "\n",
    "file = open(\"./nbmodel.txt\", \"w\")\n",
    "file.writelines(outfile[:-2])\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
